{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"edu_level\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\",\n",
    "           \"race\", \"sex\", \"hours_per_week\",\n",
    "           \"native_country\", \"income\"]\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    names=COLUMNS,\n",
    "    engine='python',\n",
    "    usecols=[0, 1, 4, 5, 6, 7, 8, 9, 12, 13, 14],\n",
    "    sep=r'\\s*,\\s*',\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "    names=COLUMNS,\n",
    "    skiprows=[0],\n",
    "    engine='python',\n",
    "    usecols=[0, 1, 4, 5, 6, 7, 8, 9, 12, 13, 14],\n",
    "    sep=r'\\s*,\\s*',\n",
    "    na_values=\"?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(how=\"any\", axis=0)\n",
    "test_df = test_df.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# To reduce the complexity, we binarize the attribute\n",
    "# To reduce the complexity, we binarize the attribute\n",
    "\n",
    "\n",
    "def mapping(tuple):\n",
    "    # age, 37\n",
    "    tuple['age'] = 1 if tuple['age'] > 37 else 0\n",
    "    # workclass\n",
    "    tuple['workclass'] = 0 if tuple['workclass'] != 'Private' else 1\n",
    "    # edu-level\n",
    "    tuple['edu_level'] = 1 if tuple['edu_level'] > 9 else 0\n",
    "    # maritial statue\n",
    "    tuple['marital_status'] = 1 if tuple['marital_status'] == \"Married-civ-spouse\" else 0\n",
    "    # occupation\n",
    "    tuple['occupation'] = 1 if tuple['occupation'] == \"Craft-repair\" else 0\n",
    "    # relationship\n",
    "    tuple['relationship'] = 0 if tuple['relationship'] == \"Not-in-family\" else 1\n",
    "    # race\n",
    "    tuple['race'] = 0 if tuple['race'] != \"White\" else 1\n",
    "    # sex\n",
    "    tuple['sex'] = 0 if tuple['sex'] != \"Male\" else 1\n",
    "    # hours per week\n",
    "    tuple['hours_per_week'] = 1 if tuple['hours_per_week'] > 40 else 0\n",
    "    # native country\n",
    "    tuple['native_country'] = 1 if tuple['native_country'] == \"United-States\" else 0\n",
    "    # income\n",
    "    tuple['income'] = 1 if tuple['income'] == '>50K' or tuple['income'] == '>50K.' else 0\n",
    "    return tuple\n",
    "\n",
    "\n",
    "train_df = train_df.apply(mapping, axis=1)\n",
    "test_df = test_df.apply(mapping, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  workclass  edu_level  marital_status  occupation  relationship  race  \\\n",
       "0     1          0          1               0           0             0     1   \n",
       "1     1          0          1               1           0             1     1   \n",
       "2     1          1          0               0           0             0     1   \n",
       "3     1          1          0               1           0             1     0   \n",
       "4     0          1          1               1           0             1     0   \n",
       "5     0          1          1               1           0             1     1   \n",
       "6     1          1          0               0           0             0     0   \n",
       "7     1          0          0               1           0             1     1   \n",
       "8     0          1          1               0           0             0     1   \n",
       "9     1          1          1               1           0             1     1   \n",
       "10    0          1          1               1           0             1     0   \n",
       "11    0          0          1               1           0             1     0   \n",
       "12    0          1          1               0           0             1     1   \n",
       "13    0          1          1               0           0             0     0   \n",
       "15    0          1          0               1           0             1     0   \n",
       "16    0          0          0               0           0             1     1   \n",
       "17    0          1          0               0           0             1     1   \n",
       "18    1          1          0               1           0             1     1   \n",
       "19    1          0          1               0           0             1     1   \n",
       "20    1          1          1               1           0             1     1   \n",
       "21    1          1          0               0           0             1     0   \n",
       "22    0          0          0               1           0             1     0   \n",
       "23    1          1          0               1           0             1     1   \n",
       "24    1          1          0               0           0             1     1   \n",
       "25    1          0          1               1           0             1     1   \n",
       "26    0          1          0               0           1             1     1   \n",
       "28    1          1          0               0           0             0     1   \n",
       "29    1          1          0               1           1             1     1   \n",
       "30    0          0          1               0           0             0     1   \n",
       "31    0          1          1               0           0             1     0   \n",
       "\n",
       "    sex  hours_per_week  native_country  income  \n",
       "0     1               0               1       0  \n",
       "1     1               0               1       0  \n",
       "2     1               0               1       0  \n",
       "3     1               0               1       0  \n",
       "4     0               0               0       0  \n",
       "5     0               0               1       0  \n",
       "6     0               0               0       0  \n",
       "7     1               1               1       1  \n",
       "8     0               1               1       1  \n",
       "9     1               0               1       1  \n",
       "10    1               1               1       1  \n",
       "11    1               0               0       1  \n",
       "12    0               0               1       0  \n",
       "13    1               1               1       0  \n",
       "15    1               1               0       0  \n",
       "16    1               0               1       0  \n",
       "17    1               0               1       0  \n",
       "18    1               1               1       0  \n",
       "19    0               1               1       1  \n",
       "20    1               1               1       1  \n",
       "21    0               0               1       0  \n",
       "22    1               0               1       0  \n",
       "23    1               0               1       0  \n",
       "24    0               0               1       0  \n",
       "25    1               0               1       1  \n",
       "26    1               0               1       0  \n",
       "28    1               1               1       0  \n",
       "29    1               0               1       0  \n",
       "30    1               1               1       0  \n",
       "31    1               1               1       0  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[0:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_df.values)\n",
    "test_data = torch.from_numpy(test_df.values)\n",
    "# merge two datasets\n",
    "dataset = torch.cat((train_data,test_data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, data_set):\n",
    "        self.x = data_set\n",
    "        self.len = data_set.size()[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "adultDataset = AdultDataset(dataset)\n",
    "dataLoader = DataLoader(dataset=adultDataset, batch_size=128, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic Generator\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, f, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size*2)\n",
    "        self.map2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        # f is action function\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        x = self.f(x)\n",
    "        return x\n",
    "\n",
    "# a basic Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, f, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        # f is action function\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return torch.sigmoid(self.map3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGAN(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(CFGAN, self).__init__()\n",
    "\n",
    "        self.age_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.workclass_net = Generator(\n",
    "            f, 5, 8, 1)\n",
    "        self.edu_level_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.marital_status_net = Generator(\n",
    "            f, 5, 8, 1)\n",
    "        self.occupation_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.relationship_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.race_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.sex_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.hours_per_week_net = Generator(\n",
    "            f, 7, 16, 1)\n",
    "        self.native_country_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.income_net = Generator(\n",
    "            f, 11, 32, 1)\n",
    "\n",
    "    def forward(self, input, intervention=-1):\n",
    "        name = [\"age\",\"workclass\",\"edu_level\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"hours_per_week\",\"native_country\",\"income\"]\n",
    "        Z = dict(zip(name, input.transpose(0, 1).view(len(name), -1, 1)))\n",
    "\n",
    "        # hight = 0 in the graph\n",
    "        # sex should considered about intervention\n",
    "        if(intervention == -1):\n",
    "            self.sex = self.race_net(Z[\"sex\"])\n",
    "        elif(intervention == 0):\n",
    "            self.sex = torch.zeros(Z[\"sex\"].size())\n",
    "        else:\n",
    "            self.sex = torch.ones(Z[\"sex\"].size())\n",
    "        self.age = self.age_net(Z[\"age\"])\n",
    "        self.race = self.sex_net(Z[\"race\"])\n",
    "        self.native_country = self.native_country_net(Z[\"native_country\"])\n",
    "\n",
    "        # hight = 1 in the graph\n",
    "        self.marital_status = self.marital_status_net(torch.cat(\n",
    "            [Z[\"marital_status\"], self.race, self.age,\n",
    "                self.sex, self.native_country], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 2 in the gragh\n",
    "        self.edu_level = self.edu_level_net(torch.cat(\n",
    "            [Z[\"edu_level\"], self.race, self.age, self.sex,\n",
    "                self.native_country, self.marital_status], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 3 in the gragh\n",
    "        self.occupation = self.occupation_net(torch.cat(\n",
    "            [Z[\"occupation\"], self.race, self.age, self.sex,\n",
    "                self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        self.hours_per_week = self.hours_per_week_net(torch.cat(\n",
    "            [Z[\"hours_per_week\"], self.race, self.age, self.sex,\n",
    "             self.native_country, self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        self.workclass = self.workclass_net(torch.cat(\n",
    "            [Z[\"workclass\"], self.age, self.marital_status,\n",
    "                self.edu_level, self.native_country], 1\n",
    "        ))\n",
    "\n",
    "        self.relationship = self.relationship_net(torch.cat(\n",
    "            [Z[\"relationship\"], self.age, self.sex, self.native_country,\n",
    "                self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 4 in the gragh\n",
    "\n",
    "        self.income = self.income_net(torch.cat(\n",
    "            [Z[\"income\"], self.race, self.age, self.sex, self.native_country, self.marital_status,\n",
    "                self.edu_level, self.occupation, self.hours_per_week, self.workclass, self.relationship], 1\n",
    "        ))\n",
    "\n",
    "        return torch.cat([self.age, self.workclass, self.edu_level, self.marital_status,\n",
    "        self.occupation, self.relationship, self.race, self.sex,\n",
    "        self.hours_per_week, self.native_country, self.income], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "g_steps = 5\n",
    "g2_steps = 50\n",
    "batch = 128\n",
    "LR = 0.001\n",
    "print_interval = 1\n",
    "# action function\n",
    "discriminator_activation_function = nn.LeakyReLU(0.2)\n",
    "generator_activation_function = torch.tanh\n",
    "\n",
    "# net init\n",
    "discriminator_1 = Discriminator(\n",
    "    discriminator_activation_function, 11, 64, 1)\n",
    "generator = CFGAN(generator_activation_function)\n",
    "\n",
    "# Binary cross entropy: https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss\n",
    "criterion = nn.BCELoss()\n",
    "# optim\n",
    "generator_optim = torch.optim.Adam(\n",
    "    generator.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "discriminator_1_optim = torch.optim.Adam(\n",
    "    discriminator_1.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug\n",
    "## test for paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = copy.copy(dataLoader)\n",
    "for real_data in data:\n",
    "\n",
    "    # 1A: Train D1 on real\n",
    "    discriminator_1.zero_grad()\n",
    "    d_real_data = real_data\n",
    "    # real data's lable should be true\n",
    "    d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "    d_real_decision = discriminator_1(d_real_data.float())\n",
    "    d_real_loss = criterion(\n",
    "        torch.squeeze(d_real_decision), d_real_labe)\n",
    "    d_real_loss.backward()\n",
    "\n",
    "    # 1B: Train D1 on fake data\n",
    "    d_fake_data = generator(torch.randn(batch, 11))\n",
    "    # print(d_fake_data.size())\n",
    "    d_fake_lable = torch.zeros(batch)\n",
    "    d_fake_decision = discriminator_1(d_fake_data)\n",
    "    d_fake_loss = criterion(torch.squeeze(\n",
    "        d_fake_decision), d_fake_lable)\n",
    "    d_fake_loss.backward()\n",
    "    # Only optimizes D1's parameters\n",
    "    discriminator_1_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.013045307248830795 0.0016629857709631324 0.01813497208058834\n",
      "1 0.006974454037845135 0.0025961659848690033 0.019351253286004066\n",
      "2 0.004128933884203434 0.0017923510167747736 0.013094463385641575\n",
      "3 0.001056463923305273 0.019427934661507607 0.012560836970806122\n",
      "4 0.00010539229697315022 0.015412031672894955 0.019025545567274094\n",
      "5 0.003513236530125141 0.0014881686074659228 0.024885153397917747\n",
      "6 0.012492464855313301 0.0033049280755221844 0.033149391412734985\n",
      "7 0.0013619082747027278 0.00015893984527792782 0.008169825188815594\n",
      "8 0.0036359746009111404 0.001317936577834189 0.044445864856243134\n",
      "9 0.0013813667465001345 0.00036774633917957544 0.004945330787450075\n",
      "10 0.013553413562476635 0.013049769215285778 0.02837757207453251\n",
      "11 0.00222163088619709 0.0010353690013289452 0.008836967870593071\n",
      "12 0.046411048620939255 0.005799140315502882 0.05417352542281151\n",
      "13 0.00038549452438019216 0.0002163913013646379 0.004485161043703556\n",
      "14 0.0003284133563283831 0.004493562504649162 0.03258446976542473\n",
      "15 0.009432760067284107 0.0024637000169605017 0.025861939415335655\n",
      "16 0.006998287979513407 0.01058128010481596 0.03379623219370842\n",
      "17 0.0038539795204997063 0.002014244208112359 0.002986673265695572\n",
      "18 0.0011297565652057528 0.00332604069262743 0.03511490300297737\n",
      "19 0.007724430877715349 0.0035026089753955603 0.014831194654107094\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(20):\n",
    "    data = copy.copy(dataLoader)\n",
    "    for real_data in data:\n",
    "\n",
    "        # 1A: Train D1 on real\n",
    "        discriminator_1.zero_grad()\n",
    "        d_real_data = real_data\n",
    "        # real data's lable should be true\n",
    "        d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "        d_real_decision = discriminator_1(d_real_data.float())\n",
    "        d_real_loss = criterion(\n",
    "            torch.squeeze(d_real_decision), d_real_labe)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # 1B: Train D1 on fake data\n",
    "        d_fake_data = generator(torch.randn(batch, 11))\n",
    "        # print(d_fake_data.size())\n",
    "        d_fake_lable = torch.zeros(batch)\n",
    "        d_fake_decision = discriminator_1(d_fake_data)\n",
    "        d_fake_loss = criterion(torch.squeeze(\n",
    "            d_fake_decision), d_fake_lable)\n",
    "        d_fake_loss.backward()\n",
    "        # Only optimizes D1's parameters\n",
    "        discriminator_1_optim.step()\n",
    "    \n",
    "    for g_index in range(64):\n",
    "        # Train G on D's response\n",
    "        generator.zero_grad()\n",
    "        g_fake_data = generator(torch.randn(batch, 11))\n",
    "        d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "        g_fake_lable = torch.ones(batch)\n",
    "        g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "        g_loss.backward()\n",
    "        generator_optim.step()\n",
    "        \n",
    "    print(i,d_real_loss.tolist(),d_fake_loss.tolist(),g_loss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9946,  1.0000, -0.1201,  0.9965,  0.9896,  1.0000,  0.9984,  1.0000,\n",
      "         -0.2794,  0.9958,  0.0368],\n",
      "        [ 0.0286,  1.0000, -0.1244,  0.9952,  0.9978,  1.0000,  0.9987,  1.0000,\n",
      "         -0.2816,  0.9958,  0.0405],\n",
      "        [ 0.9946,  1.0000,  0.8672, -0.0144,  0.9060,  1.0000,  0.9986,  1.0000,\n",
      "         -0.2575,  0.9956,  0.0565],\n",
      "        [ 0.9931,  1.0000, -0.1150,  0.9967,  0.9976,  1.0000,  0.9990,  0.9999,\n",
      "         -0.3018,  0.9961,  0.0686],\n",
      "        [ 0.0141,  1.0000,  0.9739,  0.0398,  0.9966,  0.9873,  0.9991,  0.9999,\n",
      "         -0.3041,  0.9959,  0.0626],\n",
      "        [ 0.9948,  1.0000, -0.1212,  0.8798,  0.9978,  1.0000,  0.9975,  1.0000,\n",
      "         -0.2885,  0.9957,  0.0496],\n",
      "        [ 0.9947,  1.0000,  0.9812,  0.9964,  0.9977,  1.0000,  0.9991,  1.0000,\n",
      "         -0.2805,  0.9959,  0.1102],\n",
      "        [ 0.9945,  1.0000, -0.1199,  0.9964,  0.9969,  1.0000,  0.9992,  1.0000,\n",
      "         -0.2950,  0.9959,  0.0591],\n",
      "        [ 0.9930,  1.0000,  0.9825, -0.0120,  0.9976,  1.0000,  0.9985,  1.0000,\n",
      "         -0.2663,  0.9958,  0.0963],\n",
      "        [ 0.9944,  1.0000,  0.9832,  0.9967, -0.0198,  1.0000,  0.9989,  1.0000,\n",
      "         -0.2803,  0.9957,  0.0941],\n",
      "        [ 0.9940,  1.0000, -0.1053,  0.9966, -0.1208,  1.0000,  0.9959,  1.0000,\n",
      "         -0.2783,  0.9950,  0.0519],\n",
      "        [ 0.9944,  1.0000,  0.6667,  0.9967,  0.9972,  1.0000,  0.9975,  1.0000,\n",
      "         -0.2812,  0.9947,  0.0866],\n",
      "        [ 0.1328,  1.0000,  0.9840,  0.9962,  0.9972,  1.0000,  0.9992,  1.0000,\n",
      "          0.9507,  0.9939,  0.1158],\n",
      "        [ 0.9941,  1.0000,  0.9844,  0.9959,  0.5057,  1.0000,  0.9992,  1.0000,\n",
      "         -0.2047,  0.9957,  0.0732],\n",
      "        [ 0.9942,  1.0000, -0.1088,  0.9955,  0.9979,  1.0000,  0.9992,  1.0000,\n",
      "         -0.2627,  0.9885,  0.0681],\n",
      "        [ 0.0205,  1.0000, -0.1102,  0.9962,  0.9073,  1.0000,  0.9990,  1.0000,\n",
      "         -0.2786,  0.9939,  0.0473],\n",
      "        [ 0.5928,  1.0000, -0.1131,  0.9957,  0.1095,  1.0000,  0.9723,  1.0000,\n",
      "         -0.2797,  0.9889,  0.0345],\n",
      "        [ 0.9944,  1.0000,  0.9844,  0.9963, -0.2043,  1.0000,  0.9990,  1.0000,\n",
      "         -0.2859,  0.9946,  0.0535],\n",
      "        [ 0.0430,  1.0000,  0.0724,  0.9961,  0.9976,  1.0000,  0.9984,  1.0000,\n",
      "         -0.2871,  0.9952,  0.0870],\n",
      "        [ 0.9941,  1.0000, -0.1217,  0.9961,  0.9980,  1.0000,  0.9990,  1.0000,\n",
      "         -0.2901,  0.9958,  0.0546]], grad_fn=<CatBackward>)\n",
      "tensor([[1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9993],\n",
       "        [0.9894],\n",
       "        [0.9987],\n",
       "        [0.9810],\n",
       "        [0.9668],\n",
       "        [0.9337],\n",
       "        [0.9931],\n",
       "        [0.9984],\n",
       "        [0.9879],\n",
       "        [0.9943],\n",
       "        [0.9522],\n",
       "        [0.9864],\n",
       "        [0.9685],\n",
       "        [0.9943],\n",
       "        [0.9969],\n",
       "        [0.9986],\n",
       "        [0.9935],\n",
       "        [0.9988],\n",
       "        [0.9981],\n",
       "        [0.9996],\n",
       "        [0.9996],\n",
       "        [0.9997],\n",
       "        [0.9946],\n",
       "        [0.9631],\n",
       "        [0.9949],\n",
       "        [0.9805],\n",
       "        [0.9974],\n",
       "        [0.9969],\n",
       "        [0.9868],\n",
       "        [0.9992],\n",
       "        [0.9575],\n",
       "        [0.9792],\n",
       "        [0.9777],\n",
       "        [0.9461],\n",
       "        [0.9497],\n",
       "        [0.9941],\n",
       "        [0.9995],\n",
       "        [0.9888],\n",
       "        [0.9929],\n",
       "        [0.9996],\n",
       "        [0.9934],\n",
       "        [0.9969],\n",
       "        [0.9999],\n",
       "        [0.9750],\n",
       "        [0.9804],\n",
       "        [0.9995],\n",
       "        [0.9992],\n",
       "        [0.9998],\n",
       "        [0.9383],\n",
       "        [0.9938],\n",
       "        [0.9941],\n",
       "        [0.9589],\n",
       "        [0.9681],\n",
       "        [0.9985],\n",
       "        [0.9931],\n",
       "        [0.9967],\n",
       "        [0.9928],\n",
       "        [0.9993],\n",
       "        [0.9670],\n",
       "        [0.9827],\n",
       "        [0.9957],\n",
       "        [0.9303],\n",
       "        [0.9995],\n",
       "        [0.9994],\n",
       "        [0.9873],\n",
       "        [0.9947],\n",
       "        [0.9951],\n",
       "        [0.9880],\n",
       "        [0.9709],\n",
       "        [0.9728],\n",
       "        [0.9996],\n",
       "        [0.9966],\n",
       "        [0.9993],\n",
       "        [0.9985],\n",
       "        [0.9486],\n",
       "        [0.9769],\n",
       "        [0.9991],\n",
       "        [0.9943],\n",
       "        [0.9929],\n",
       "        [0.9779],\n",
       "        [0.9996],\n",
       "        [0.9738],\n",
       "        [0.9902],\n",
       "        [0.9643],\n",
       "        [0.9968],\n",
       "        [0.9939],\n",
       "        [0.9975],\n",
       "        [0.9942],\n",
       "        [0.9968],\n",
       "        [0.9907],\n",
       "        [0.9943],\n",
       "        [0.9998],\n",
       "        [0.9994],\n",
       "        [0.9926],\n",
       "        [0.9992],\n",
       "        [0.9968],\n",
       "        [0.9999],\n",
       "        [0.9824],\n",
       "        [0.9888],\n",
       "        [0.9981],\n",
       "        [0.9995],\n",
       "        [0.9972],\n",
       "        [0.9620],\n",
       "        [0.9899],\n",
       "        [0.9821],\n",
       "        [0.9795],\n",
       "        [0.9975],\n",
       "        [1.0000],\n",
       "        [0.9034],\n",
       "        [0.9940],\n",
       "        [0.9985],\n",
       "        [0.9569],\n",
       "        [0.9338],\n",
       "        [0.9989],\n",
       "        [0.9127],\n",
       "        [0.9903],\n",
       "        [0.9437],\n",
       "        [0.9395],\n",
       "        [0.9995],\n",
       "        [0.9958],\n",
       "        [0.9372],\n",
       "        [0.9958],\n",
       "        [0.9987],\n",
       "        [0.9882],\n",
       "        [0.9644],\n",
       "        [0.9673],\n",
       "        [0.9755],\n",
       "        [0.9736]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = generator(torch.randn(20, 11))\n",
    "print(f)\n",
    "print(f.ge(0.5).long())\n",
    "dis = discriminator_1(generator(torch.randn(batch, 11)))\n",
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    " for g_index in range(32):\n",
    "    # Train G on D's response\n",
    "    generator.zero_grad()\n",
    "    g_fake_data = generator(torch.randn(batch, 11))\n",
    "    d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "    g_fake_lable = torch.ones(batch)\n",
    "    g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "    g_loss.backward()\n",
    "    generator_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D (0.22125402092933655 real_err, 0.059217192232608795 fake_err) G_l (0.015695534646511078 err) G_0l (None) G_1l (None) G_2l (0.07190127670764923) G_3l (0.07072243094444275);\n",
      "Epoch 1: D (0.8784480094909668 real_err, 0.14986123144626617 fake_err) G_l (0.05311468243598938 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.04634556174278259);\n",
      "Epoch 2: D (0.32248008251190186 real_err, 0.18921847641468048 fake_err) G_l (0.010749738663434982 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.06491663306951523);\n",
      "Epoch 3: D (0.2830143868923187 real_err, 0.25805848836898804 fake_err) G_l (0.02052653394639492 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.029447432607412338);\n",
      "Epoch 4: D (0.08148318529129028 real_err, 0.15486261248588562 fake_err) G_l (0.0037686177529394627 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.09675496816635132);\n",
      "Epoch 5: D (0.6273917555809021 real_err, 0.5055474638938904 fake_err) G_l (0.02917487919330597 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.030271584168076515);\n",
      "Epoch 6: D (0.49867790937423706 real_err, 0.5175061225891113 fake_err) G_l (0.3499322235584259 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.0468820184469223);\n",
      "Epoch 7: D (0.4130099415779114 real_err, 0.2603578567504883 fake_err) G_l (0.0016336796106770635 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.08965965360403061);\n",
      "Epoch 8: D (0.5100611448287964 real_err, 0.34230339527130127 fake_err) G_l (0.10457960516214371 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.01652887836098671);\n",
      "Epoch 9: D (0.5397396683692932 real_err, 0.16634927690029144 fake_err) G_l (0.0007370166131295264 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.028134718537330627);\n",
      "Epoch 10: D (0.4859308898448944 real_err, 0.34623533487319946 fake_err) G_l (0.00014762203500140458 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.32949814200401306);\n",
      "Epoch 11: D (0.2175847291946411 real_err, 0.17840856313705444 fake_err) G_l (0.06142515689134598 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.25313279032707214);\n",
      "Epoch 12: D (0.5938618183135986 real_err, 0.30312445759773254 fake_err) G_l (0.0015326347202062607 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.5732567310333252);\n",
      "Epoch 13: D (0.3207414150238037 real_err, 0.13182073831558228 fake_err) G_l (0.05894848331809044 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.5374412536621094);\n",
      "Epoch 14: D (0.16929370164871216 real_err, 0.1976919323205948 fake_err) G_l (0.47929638624191284 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.3470424711704254);\n",
      "Epoch 15: D (0.11231118440628052 real_err, 0.048068709671497345 fake_err) G_l (4.9841088184621185e-05 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.6881944537162781);\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3e16d4c23e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mg_fake_lable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_g_fake_decision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_fake_lable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mgenerator_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "for epoch in range(num_epochs):\n",
    "    # GAN1\n",
    "    # 1. Train D on real+fake\n",
    "    # D.zero_grad()\n",
    "    data = copy.copy(dataLoader)\n",
    "\n",
    "    for real_data in data:\n",
    "\n",
    "        # 1A: Train D1 on real\n",
    "        discriminator_1.zero_grad()\n",
    "        d_real_data = real_data\n",
    "        # real data's lable should be true\n",
    "        d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "        d_real_decision = discriminator_1(d_real_data.float())\n",
    "        d_real_loss = criterion(\n",
    "            torch.squeeze(d_real_decision), d_real_labe)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # 1B: Train D1 on fake data\n",
    "        d_fake_data = generator(torch.randn(batch, 11))\n",
    "        # print(d_fake_data.size())\n",
    "        d_fake_lable = torch.zeros(batch)\n",
    "        d_fake_decision = discriminator_1(d_fake_data)\n",
    "        d_fake_loss = criterion(torch.squeeze(\n",
    "            d_fake_decision), d_fake_lable)\n",
    "        d_fake_loss.backward()\n",
    "        # Only optimizes D1's parameters\n",
    "        discriminator_1_optim.step()\n",
    "\n",
    "        drl, dfl = d_real_loss.tolist(), d_fake_loss.tolist()\n",
    "    for g_index in range(g_steps):\n",
    "        # Train G on D's response\n",
    "        generator.zero_grad()\n",
    "\n",
    "        g_fake_data = generator(torch.randn(batch, 11))\n",
    "        d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "        g_fake_lable = torch.ones(batch)\n",
    "        g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "        g_loss.backward()\n",
    "        generator_optim.step()\n",
    "        gl = g_loss.tolist()\n",
    "\n",
    "    # GAN2\n",
    "    for g_index in range(g2_steps):\n",
    "        generator.zero_grad()\n",
    "        noise_z = torch.randn(batch, 11)\n",
    "        fake_data = generator(noise_z)\n",
    "        # O = {race; native country}:(0,0) (0,1) (1,0) (1,1)\n",
    "        noise_o0 = []\n",
    "        noise_o1 = []\n",
    "        noise_o2 = []\n",
    "        noise_o3 = []\n",
    "\n",
    "        for index, single_data in enumerate(fake_data):\n",
    "            if(single_data[7] < 0.5 and single_data[9] < 0.5):\n",
    "                noise_o0.append(noise_z[index].view(1, -1))\n",
    "            elif(single_data[7] < 0.5 and single_data[9] >= 0.5):\n",
    "                noise_o1.append(noise_z[index].view(1, -1))\n",
    "            elif(single_data[7] >= 0.5 and single_data[9] < 0.5):\n",
    "                noise_o2.append(noise_z[index].view(1, -1))\n",
    "            else:\n",
    "                noise_o3.append(noise_z[index].view(1, -1))\n",
    "        ge0,ge1,ge2,ge3 = None,None,None,None\n",
    "        if(len(noise_o0) != 0):\n",
    "            noise_o0 = torch.cat(noise_o0)\n",
    "            o0_0_lable = generator(noise_o0, 0)[:, -1]\n",
    "            o0_1_lable = generator(noise_o0, 1)[:, -1].detach()\n",
    "            g_error0 = criterion(o0_0_lable, o0_1_lable)\n",
    "            g_error0.backward()\n",
    "            ge0 = g_error0.tolist()\n",
    "        if(len(noise_o1) != 0):\n",
    "            noise_o1 = torch.cat(noise_o1)\n",
    "            o1_0_lable = generator(noise_o1, 0)[:, -1]\n",
    "            o1_1_lable = generator(noise_o1, 1)[:, -1].detach()\n",
    "            g_error1 = criterion(o1_0_lable, o1_1_lable)\n",
    "            g_error1.backward()\n",
    "            ge1 = g_error1.tolist()\n",
    "        if(len(noise_o2) != 0):\n",
    "            noise_o2 = torch.cat(noise_o2)\n",
    "            o2_0_lable = generator(noise_o2, 0)[:, -1]\n",
    "            o2_1_lable = generator(noise_o2, 1)[:, -1].detach()\n",
    "            g_error2 = criterion(o2_0_lable, o2_1_lable)\n",
    "            g_error2.backward()\n",
    "            ge2 = g_error2.tolist()\n",
    "        if(len(noise_o3) != 0):\n",
    "            noise_o3 = torch.cat(noise_o3)\n",
    "            o3_0_lable = generator(noise_o3, 0)[:, -1]\n",
    "            o3_1_lable = generator(noise_o3, 1)[:, -1].detach()\n",
    "            g_error3 = criterion(o3_0_lable, o3_1_lable)\n",
    "            g_error3.backward()\n",
    "            ge3 = g_error3.tolist()\n",
    "\n",
    "        generator_optim.step()\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"Epoch %s: D (%s real_err, %s fake_err) G_l (%s err) G_0l (%s) G_1l (%s) G_2l (%s) G_3l (%s);\" % (\n",
    "            epoch, drl, dfl, gl, ge0, ge1, ge2, ge3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': conda)",
   "language": "python",
   "name": "python37464bitanaconda3conda5df9da8887984ee4b635526a125f85b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
