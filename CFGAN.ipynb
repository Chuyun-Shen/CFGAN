{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNS = [\"age\", \"workclass\", \"edu_level\",\n",
    "           \"marital_status\", \"occupation\", \"relationship\",\n",
    "           \"race\", \"sex\", \"hours_per_week\",\n",
    "           \"native_country\", \"income\"]\n",
    "\n",
    "train_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\",\n",
    "    names=COLUMNS,\n",
    "    engine='python',\n",
    "    usecols=[0, 1, 4, 5, 6, 7, 8, 9, 12, 13, 14],\n",
    "    sep=r'\\s*,\\s*',\n",
    "    na_values=\"?\"\n",
    ")\n",
    "\n",
    "test_df = pd.read_csv(\n",
    "    filepath_or_buffer=\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\",\n",
    "    names=COLUMNS,\n",
    "    skiprows=[0],\n",
    "    engine='python',\n",
    "    usecols=[0, 1, 4, 5, 6, 7, 8, 9, 12, 13, 14],\n",
    "    sep=r'\\s*,\\s*',\n",
    "    na_values=\"?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "train_df = train_df.dropna(how=\"any\", axis=0)\n",
    "test_df = test_df.dropna(how=\"any\", axis=0)\n",
    "\n",
    "# To reduce the complexity, we binarize the attribute\n",
    "# To reduce the complexity, we binarize the attribute\n",
    "\n",
    "\n",
    "def mapping(tuple):\n",
    "    # age, 37\n",
    "    tuple['age'] = 1 if tuple['age'] > 37 else 0\n",
    "    # workclass\n",
    "    tuple['workclass'] = 0 if tuple['workclass'] != 'Private' else 1\n",
    "    # edu-level\n",
    "    tuple['edu_level'] = 1 if tuple['edu_level'] > 9 else 0\n",
    "    # maritial statue\n",
    "    tuple['marital_status'] = 1 if tuple['marital_status'] == \"Married-civ-spouse\" else 0\n",
    "    # occupation\n",
    "    tuple['occupation'] = 1 if tuple['occupation'] == \"Craft-repair\" else 0\n",
    "    # relationship\n",
    "    tuple['relationship'] = 0 if tuple['relationship'] == \"Not-in-family\" else 1\n",
    "    # race\n",
    "    tuple['race'] = 0 if tuple['race'] != \"White\" else 1\n",
    "    # sex\n",
    "    tuple['sex'] = 0 if tuple['sex'] != \"Male\" else 1\n",
    "    # hours per week\n",
    "    tuple['hours_per_week'] = 1 if tuple['hours_per_week'] > 40 else 0\n",
    "    # native country\n",
    "    tuple['native_country'] = 1 if tuple['native_country'] == \"United-States\" else 0\n",
    "    # income\n",
    "    tuple['income'] = 1 if tuple['income'] == '>50K' or tuple['income'] == '>50K.' else 0\n",
    "    return tuple\n",
    "\n",
    "\n",
    "train_df = train_df.apply(mapping, axis=1)\n",
    "test_df = test_df.apply(mapping, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>edu_level</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  workclass  edu_level  marital_status  occupation  relationship  \\\n",
       "43     1          1          0               0           0             1   \n",
       "44     0          1          0               0           0             0   \n",
       "45     1          0          1               1           0             1   \n",
       "46     1          1          0               1           0             1   \n",
       "47     1          1          1               0           0             1   \n",
       "48     1          0          1               1           1             1   \n",
       "49     0          1          1               0           0             0   \n",
       "50     0          1          1               1           0             1   \n",
       "52     1          1          1               1           0             1   \n",
       "53     1          0          1               0           0             0   \n",
       "54     1          0          0               0           0             0   \n",
       "55     1          1          1               1           0             1   \n",
       "56     1          1          0               1           0             1   \n",
       "57     0          1          1               1           0             1   \n",
       "58     1          1          0               1           0             1   \n",
       "59     0          1          0               1           0             1   \n",
       "60     0          1          1               1           0             1   \n",
       "62     1          1          0               1           0             1   \n",
       "63     1          1          1               1           0             1   \n",
       "64     0          1          1               0           0             0   \n",
       "65     0          1          0               1           1             1   \n",
       "66     0          1          1               0           0             0   \n",
       "67     1          1          0               1           0             1   \n",
       "68     1          0          1               1           0             1   \n",
       "70     0          1          1               0           0             1   \n",
       "71     0          1          1               0           0             1   \n",
       "72     0          0          1               1           0             1   \n",
       "73     0          1          1               0           0             0   \n",
       "74     1          1          1               1           0             1   \n",
       "75     0          1          0               0           0             1   \n",
       "76     1          1          1               1           0             1   \n",
       "78     0          1          0               0           0             1   \n",
       "79     0          0          0               1           0             1   \n",
       "80     0          1          0               0           0             0   \n",
       "81     1          1          1               1           0             1   \n",
       "82     1          1          0               1           0             1   \n",
       "83     1          1          0               1           0             1   \n",
       "84     1          1          0               0           1             0   \n",
       "85     1          1          0               0           0             1   \n",
       "86     1          0          0               1           0             1   \n",
       "87     0          1          1               1           0             1   \n",
       "88     0          1          0               0           0             0   \n",
       "89     1          0          1               0           0             0   \n",
       "90     1          1          1               1           0             1   \n",
       "91     0          1          1               0           1             1   \n",
       "92     0          1          1               0           0             1   \n",
       "94     0          0          1               1           0             1   \n",
       "95     0          0          1               0           0             0   \n",
       "96     1          0          1               1           0             1   \n",
       "97     0          1          1               1           0             1   \n",
       "98     1          1          1               0           0             1   \n",
       "99     0          0          0               0           0             1   \n",
       "100    1          1          1               1           0             1   \n",
       "101    1          1          1               1           0             1   \n",
       "102    1          0          1               0           0             0   \n",
       "103    0          1          1               0           0             1   \n",
       "104    0          1          0               0           1             0   \n",
       "105    0          0          0               1           1             1   \n",
       "107    0          1          0               0           0             1   \n",
       "108    0          0          0               0           0             0   \n",
       "\n",
       "     race  sex  hours_per_week  native_country  income  \n",
       "43      1    0               0               1       0  \n",
       "44      1    1               0               1       0  \n",
       "45      0    1               0               1       1  \n",
       "46      1    1               0               1       0  \n",
       "47      1    0               0               1       0  \n",
       "48      1    1               0               1       0  \n",
       "49      1    1               1               1       0  \n",
       "50      0    0               0               1       0  \n",
       "52      1    0               1               0       1  \n",
       "53      1    1               1               1       1  \n",
       "54      1    1               1               1       0  \n",
       "55      1    1               0               1       1  \n",
       "56      1    1               0               0       0  \n",
       "57      1    1               0               0       0  \n",
       "58      1    1               1               1       0  \n",
       "59      1    1               0               1       0  \n",
       "60      1    1               0               1       0  \n",
       "62      1    1               0               1       0  \n",
       "63      1    1               1               1       1  \n",
       "64      1    1               1               1       0  \n",
       "65      1    1               0               1       0  \n",
       "66      1    0               0               1       0  \n",
       "67      1    0               0               1       1  \n",
       "68      1    1               1               1       1  \n",
       "70      1    1               0               1       0  \n",
       "71      0    0               0               1       0  \n",
       "72      1    1               1               1       1  \n",
       "73      1    1               0               1       0  \n",
       "74      1    1               0               1       0  \n",
       "75      1    1               0               0       0  \n",
       "76      1    1               0               1       0  \n",
       "78      1    0               0               1       0  \n",
       "79      1    1               0               1       0  \n",
       "80      1    1               0               1       0  \n",
       "81      1    1               0               0       0  \n",
       "82      1    0               0               1       0  \n",
       "83      1    1               1               1       0  \n",
       "84      1    0               0               1       1  \n",
       "85      1    0               0               1       0  \n",
       "86      1    1               0               1       1  \n",
       "87      1    1               1               1       0  \n",
       "88      1    1               0               1       0  \n",
       "89      1    0               1               1       1  \n",
       "90      1    1               0               1       0  \n",
       "91      1    0               0               1       0  \n",
       "92      0    0               0               1       0  \n",
       "94      1    1               0               1       1  \n",
       "95      1    1               1               1       0  \n",
       "96      1    1               1               1       1  \n",
       "97      1    1               1               1       1  \n",
       "98      1    0               0               0       0  \n",
       "99      0    1               0               1       0  \n",
       "100     1    1               0               1       1  \n",
       "101     1    1               1               1       1  \n",
       "102     1    0               1               1       0  \n",
       "103     1    0               0               1       0  \n",
       "104     1    1               0               1       0  \n",
       "105     1    1               0               1       1  \n",
       "107     1    1               0               1       0  \n",
       "108     1    0               0               1       0  "
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[40:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.from_numpy(train_df.values)\n",
    "test_data = torch.from_numpy(test_df.values)\n",
    "# merge two datasets\n",
    "dataset = torch.cat((train_data,test_data), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "class AdultDataset(Dataset):\n",
    "    def __init__(self, data_set):\n",
    "        self.x = data_set\n",
    "        self.len = data_set.size()[0]\n",
    "    def __getitem__(self,index):\n",
    "        return self.x[index]\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "adultDataset = AdultDataset(dataset)\n",
    "dataLoader = DataLoader(dataset=adultDataset, batch_size=128, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic Generator\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, f, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size*2)\n",
    "        self.map2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        # f is action function\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map2(x)\n",
    "        x = self.f(x)\n",
    "        x = self.map3(x)\n",
    "        x = self.f(x)\n",
    "        return x\n",
    "\n",
    "# a basic Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, f, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size*2)\n",
    "        self.map2 = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "        # f is action function\n",
    "        self.f = f\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(self.map1(x))\n",
    "        x = self.f(self.map2(x))\n",
    "        return torch.sigmoid(self.map3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFGAN(nn.Module):\n",
    "    def __init__(self, f):\n",
    "        super(CFGAN, self).__init__()\n",
    "\n",
    "        self.age_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.workclass_net = Generator(\n",
    "            f, 5, 16, 1)\n",
    "        self.edu_level_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.marital_status_net = Generator(\n",
    "            f, 5, 16, 1)\n",
    "        self.occupation_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.relationship_net = Generator(\n",
    "            f, 6, 16, 1)\n",
    "        self.race_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.sex_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.hours_per_week_net = Generator(\n",
    "            f, 7, 16, 1)\n",
    "        self.native_country_net = Generator(\n",
    "            f, 1, 4, 1)\n",
    "        self.income_net = Generator(\n",
    "            f, 11, 32, 1)\n",
    "\n",
    "    def forward(self, input, intervention=-1):\n",
    "        name = [\"age\",\"workclass\",\"edu_level\",\"marital_status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"hours_per_week\",\"native_country\",\"income\"]\n",
    "        Z = dict(zip(name, input.transpose(0, 1).view(len(name), -1, 1)))\n",
    "\n",
    "        # hight = 0 in the graph\n",
    "        # sex should considered about intervention\n",
    "        if(intervention == -1):\n",
    "            self.sex = self.race_net(Z[\"sex\"])\n",
    "        elif(intervention == 0):\n",
    "            self.sex = torch.zeros(Z[\"sex\"].size())\n",
    "        else:\n",
    "            self.sex = torch.ones(Z[\"sex\"].size())\n",
    "        self.age = self.age_net(Z[\"age\"])\n",
    "        self.race = self.sex_net(Z[\"race\"])\n",
    "        self.native_country = self.native_country_net(Z[\"native_country\"])\n",
    "\n",
    "        # hight = 1 in the graph\n",
    "        self.marital_status = self.marital_status_net(torch.cat(\n",
    "            [Z[\"marital_status\"], self.race, self.age,\n",
    "                self.sex, self.native_country], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 2 in the gragh\n",
    "        self.edu_level = self.edu_level_net(torch.cat(\n",
    "            [Z[\"edu_level\"], self.race, self.age, self.sex,\n",
    "                self.native_country, self.marital_status], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 3 in the gragh\n",
    "        self.occupation = self.occupation_net(torch.cat(\n",
    "            [Z[\"occupation\"], self.race, self.age, self.sex,\n",
    "                self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        self.hours_per_week = self.hours_per_week_net(torch.cat(\n",
    "            [Z[\"hours_per_week\"], self.race, self.age, self.sex,\n",
    "             self.native_country, self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        self.workclass = self.workclass_net(torch.cat(\n",
    "            [Z[\"workclass\"], self.age, self.marital_status,\n",
    "                self.edu_level, self.native_country], 1\n",
    "        ))\n",
    "\n",
    "        self.relationship = self.relationship_net(torch.cat(\n",
    "            [Z[\"relationship\"], self.age, self.sex, self.native_country,\n",
    "                self.marital_status, self.edu_level], 1\n",
    "        ))\n",
    "\n",
    "        # hight = 4 in the gragh\n",
    "\n",
    "        self.income = self.income_net(torch.cat(\n",
    "            [Z[\"income\"], self.race, self.age, self.sex, self.native_country, self.marital_status,\n",
    "                self.edu_level, self.occupation, self.hours_per_week, self.workclass, self.relationship], 1\n",
    "        ))\n",
    "\n",
    "        return torch.cat([self.age, self.workclass, self.edu_level, self.marital_status,\n",
    "        self.occupation, self.relationship, self.race, self.sex,\n",
    "        self.hours_per_week, self.native_country, self.income], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "g_steps = 5\n",
    "g2_steps = 50\n",
    "batch = 256\n",
    "LR = 0.0005\n",
    "print_interval = 1\n",
    "# action function\n",
    "discriminator_activation_function = nn.LeakyReLU(0.2)\n",
    "generator_activation_function = torch.tanh\n",
    "\n",
    "# net init\n",
    "discriminator_1 = Discriminator(\n",
    "    discriminator_activation_function, 11, 64, 1)\n",
    "generator = CFGAN(generator_activation_function)\n",
    "\n",
    "# Binary cross entropy: https://pytorch.org/docs/stable/nn.html?highlight=bceloss#torch.nn.BCELoss\n",
    "criterion = nn.BCELoss()\n",
    "# optim\n",
    "generator_optim = torch.optim.Adam(\n",
    "    generator.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "discriminator_1_optim = torch.optim.Adam(\n",
    "    discriminator_1.parameters(), lr=LR, betas=(0.9, 0.99))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# debug\n",
    "## test for paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = copy.copy(dataLoader)\n",
    "for real_data in data:\n",
    "\n",
    "    # 1A: Train D1 on real\n",
    "    discriminator_1.zero_grad()\n",
    "    d_real_data = real_data\n",
    "    # real data's lable should be true\n",
    "    d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "    d_real_decision = discriminator_1(d_real_data.float())\n",
    "    d_real_loss = criterion(\n",
    "        torch.squeeze(d_real_decision), d_real_labe)\n",
    "    d_real_loss.backward()\n",
    "\n",
    "    # 1B: Train D1 on fake data\n",
    "    d_fake_data = generator(torch.randn(batch, 11))\n",
    "    # print(d_fake_data.size())\n",
    "    d_fake_lable = torch.zeros(batch)\n",
    "    d_fake_decision = discriminator_1(d_fake_data)\n",
    "    d_fake_loss = criterion(torch.squeeze(\n",
    "        d_fake_decision), d_fake_lable)\n",
    "    d_fake_loss.backward()\n",
    "    # Only optimizes D1's parameters\n",
    "    discriminator_1_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.01746196672320366 0.01706872694194317 0.031115896999835968\n",
      "1 0.006720302160829306 0.011945049278438091 0.014585972763597965\n",
      "2 0.009522311389446259 0.014734037220478058 0.019268788397312164\n",
      "3 0.012144551612436771 0.009390733204782009 0.0056714885868132114\n",
      "4 0.007415828760713339 0.02160409651696682 0.02024024911224842\n",
      "5 0.008018492721021175 0.009443645365536213 0.007449044380337\n",
      "6 0.0066832611337304115 0.0101286880671978 0.019743988290429115\n",
      "7 0.00682487990707159 0.007412943057715893 0.005183602217584848\n",
      "8 0.012305964715778828 0.011076987721025944 0.016665050759911537\n",
      "9 0.012146513909101486 0.007454392965883017 0.008507216349244118\n",
      "10 0.009904150851070881 0.01626337319612503 0.011868362314999104\n",
      "11 0.0031741480343043804 0.006769614759832621 0.005074944347143173\n",
      "12 0.009624160826206207 0.023218706250190735 0.020819557830691338\n",
      "13 0.005209844559431076 0.0037685297429561615 0.00498428987339139\n",
      "14 0.006463318597525358 0.008819002658128738 0.015797097235918045\n",
      "15 0.005348532926291227 0.004320316016674042 0.004177142400294542\n",
      "16 0.008844413794577122 0.010929265059530735 0.018641265109181404\n",
      "17 0.004095637239515781 0.0031115009915083647 0.003133346326649189\n",
      "18 0.00554632255807519 0.004468014929443598 0.013437490910291672\n",
      "19 0.009874214418232441 0.007076919078826904 0.0059922561049461365\n",
      "20 0.01045250240713358 0.004172324202954769 0.014791378751397133\n",
      "21 0.003885646117851138 0.005278062541037798 0.004837143700569868\n",
      "22 0.005212012678384781 0.006663732696324587 0.013979531824588776\n",
      "23 0.001992789562791586 0.004642131272703409 0.005546675994992256\n",
      "24 0.006423755548894405 0.003329105209559202 0.013832923024892807\n",
      "25 0.003110703779384494 0.003990840166807175 0.003432649653404951\n",
      "26 0.010958925820887089 0.003682113951072097 0.016263356432318687\n",
      "27 0.00460040383040905 0.0040082125924527645 0.004562415648251772\n",
      "28 0.0056592971086502075 0.004443589597940445 0.01760968007147312\n",
      "29 0.0019049685215577483 0.003536132164299488 0.0034664710983633995\n",
      "30 0.004717569332569838 0.040028173476457596 0.016359886154532433\n",
      "31 0.002446030732244253 0.0034140434581786394 0.0031003430485725403\n",
      "32 0.004000501241534948 0.003060410963371396 0.016095638275146484\n",
      "33 0.0031855248380452394 0.0024714653845876455 0.0027490241918712854\n",
      "34 0.0155367786064744 0.013407954946160316 0.02691703476011753\n",
      "35 0.0010166928404942155 0.0021857235115021467 0.00226290593855083\n",
      "36 0.0033274556044489145 0.014847734943032265 0.020348630845546722\n",
      "37 0.0029340393375605345 0.0024524761829525232 0.0033582441974431276\n",
      "38 0.010489062406122684 0.0016585869016125798 0.024975016713142395\n",
      "39 0.001411008182913065 0.0012679208302870393 0.0017934484640136361\n",
      "40 0.0032185360323637724 0.005797780584543943 0.02095731534063816\n",
      "41 0.004394407384097576 0.004125216510146856 0.004416264593601227\n",
      "42 0.00386505457572639 0.0034472390543669462 0.018071303144097328\n",
      "43 0.0011566599132493138 0.0018263281090185046 0.002711143111810088\n",
      "44 0.0037591061554849148 0.0032365857623517513 0.012201063334941864\n",
      "45 0.004928635898977518 0.00393914058804512 0.004876725375652313\n",
      "46 0.004896912723779678 0.0281264279037714 0.010400473140180111\n",
      "47 0.0019643709529191256 0.0020091980695724487 0.0031280324328690767\n",
      "48 0.0051031773909926414 0.0021956132259219885 0.010332930833101273\n",
      "49 0.0038254489190876484 0.0039568496868014336 0.003056225599721074\n",
      "50 0.0008786667603999376 0.0018731066957116127 0.007060995325446129\n",
      "51 0.002929831389337778 0.002522316761314869 0.0027946466580033302\n",
      "52 0.009096815250813961 0.011347604915499687 0.017169641330838203\n",
      "53 0.003156939521431923 0.0019846800714731216 0.002028378890827298\n",
      "54 0.002148570492863655 0.0016399635933339596 0.00938769243657589\n",
      "55 0.0017331864219158888 0.0021687729749828577 0.0018678897758945823\n",
      "56 0.0029761355835944414 0.002774498425424099 0.015455564484000206\n",
      "57 0.0016210260801017284 0.0016245938604697585 0.001916737062856555\n",
      "58 0.0022685073781758547 0.0026951711624860764 0.013029415160417557\n",
      "59 0.0015494739636778831 0.0013792423997074366 0.0017861223313957453\n",
      "60 0.005493787582963705 0.02684163860976696 0.014847300946712494\n",
      "61 0.0009928749641403556 0.0011619162978604436 0.0016723948065191507\n",
      "62 0.003763591405004263 0.024723313748836517 0.01388936024159193\n",
      "63 0.0012462316080927849 0.0019733747467398643 0.002413750160485506\n",
      "64 0.003913798835128546 0.015351113863289356 0.013494573533535004\n",
      "65 0.001549988053739071 0.0017639374127611518 0.0016271363710984588\n",
      "66 0.007046174257993698 0.0030661977361887693 0.014178930781781673\n",
      "67 0.0008676348952576518 0.001162470318377018 0.0015594986034557223\n",
      "68 0.00277095683850348 0.0014249886153265834 0.013370760716497898\n",
      "69 0.0019527791300788522 0.0014413207536563277 0.0016462645726278424\n",
      "70 0.0053425077348947525 0.004618901759386063 0.01728142239153385\n",
      "71 0.0008946930756792426 0.0007396233268082142 0.001399992499500513\n",
      "72 0.003851650981232524 0.0034164818935096264 0.017271799966692924\n",
      "73 0.0022362626623362303 0.001349210157059133 0.002028021262958646\n",
      "74 0.0071113198064267635 0.0022232229821383953 0.017816171050071716\n",
      "75 0.0008844073745422065 0.0005593823734670877 0.0014451596653088927\n",
      "76 0.002339692786335945 0.0016844026977196336 0.013107444159686565\n",
      "77 0.0008336822502315044 0.001120869186706841 0.0017421272350475192\n",
      "78 0.005674630403518677 0.0019784916657954454 0.023213541135191917\n",
      "79 0.0010984293185174465 0.0012667125556617975 0.0016394976992160082\n",
      "80 0.0012631312711164355 0.002203152747824788 0.010262434370815754\n",
      "81 0.001020896714180708 0.0007383282645605505 0.0015336875803768635\n",
      "82 0.007028659805655479 0.014612752944231033 0.017170075327157974\n",
      "83 0.0003910928498953581 0.000692760047968477 0.0014399250503629446\n",
      "84 0.005781090352684259 0.002316354773938656 0.013618385419249535\n",
      "85 0.0020911102183163166 0.001102244365029037 0.0017405117396265268\n",
      "86 0.004755496047437191 0.0031397815328091383 0.01710696332156658\n",
      "87 0.0012828471371904016 0.0009564002975821495 0.0016738135600462556\n",
      "88 0.0026523962151259184 0.0034948347602039576 0.014439418911933899\n",
      "89 0.0008427926222793758 0.00068918481701985 0.0012290969025343657\n",
      "90 0.003661631839349866 0.0030356994830071926 0.011985285207629204\n",
      "91 0.0015573222190141678 0.0012843097792938352 0.0017401556251570582\n",
      "92 0.002670667599886656 0.012188989669084549 0.013586195185780525\n",
      "93 0.002135071437805891 0.0014813654124736786 0.0022576709743589163\n",
      "94 0.003690039739012718 0.018682779744267464 0.01197599433362484\n",
      "95 0.0015258709900081158 0.0021251088473945856 0.0019538700580596924\n",
      "96 0.0033391450997442007 0.002528984099626541 0.015453151427209377\n",
      "97 0.0013328023487702012 0.0008231409010477364 0.0016530186403542757\n",
      "98 0.001362955430522561 0.005589820444583893 0.012095152400434017\n",
      "99 0.0007623432320542634 0.0006323917186819017 0.0014872306492179632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(100):\n",
    "    data = copy.copy(dataLoader)\n",
    "    for real_data in data:\n",
    "\n",
    "        # 1A: Train D1 on real\n",
    "        discriminator_1.zero_grad()\n",
    "        d_real_data = real_data\n",
    "        # real data's lable should be true\n",
    "        d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "        d_real_decision = discriminator_1(d_real_data.float())\n",
    "        d_real_loss = criterion(\n",
    "            torch.squeeze(d_real_decision), d_real_labe)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # 1B: Train D1 on fake data\n",
    "        d_fake_data = generator(torch.randn(batch, 11))\n",
    "        # print(d_fake_data.size())\n",
    "        d_fake_lable = torch.zeros(batch)\n",
    "        d_fake_decision = discriminator_1(d_fake_data)\n",
    "        d_fake_loss = criterion(torch.squeeze(\n",
    "            d_fake_decision), d_fake_lable)\n",
    "        d_fake_loss.backward()\n",
    "        # Only optimizes D1's parameters\n",
    "        discriminator_1_optim.step()\n",
    "    \n",
    "    for g_index in range(64):\n",
    "        # Train G on D's response\n",
    "        generator.zero_grad()\n",
    "        g_fake_data = generator(torch.randn(batch, 11))\n",
    "        d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "        g_fake_lable = torch.ones(batch)\n",
    "        g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "        g_loss.backward()\n",
    "        generator_optim.step()\n",
    "        \n",
    "    print(i,d_real_loss.tolist(),d_fake_loss.tolist(),g_loss.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000, -0.0471,  0.9994,  0.9999, -0.0160,  1.0000,  0.9984,  0.9997,\n",
      "          0.9998,  1.0000, -0.0273],\n",
      "        [ 1.0000, -0.0421, -0.0383,  0.9999, -0.0126,  1.0000,  0.9975,  0.9997,\n",
      "          0.0088,  1.0000, -0.0260],\n",
      "        [ 1.0000, -0.0480,  0.9994, -0.1612, -0.0184,  0.1952,  0.9981,  0.9997,\n",
      "          0.9998,  1.0000,  0.9511],\n",
      "        [ 1.0000, -0.0485,  0.9994, -0.1732, -0.0192,  0.3747,  0.9984,  0.9998,\n",
      "          0.9998,  1.0000, -0.0283],\n",
      "        [ 1.0000,  0.9986, -0.0057,  0.9999, -0.0129,  1.0000,  0.9954,  0.9997,\n",
      "          0.9998,  1.0000, -0.0269],\n",
      "        [ 1.0000, -0.0470,  0.9994,  0.9994, -0.0167,  1.0000,  0.9970,  0.9997,\n",
      "          0.0085,  1.0000, -0.0252],\n",
      "        [ 1.0000,  0.9986,  0.9994,  0.9999, -0.0156,  1.0000,  0.9942,  0.9997,\n",
      "          0.9998,  1.0000,  0.9994],\n",
      "        [ 1.0000,  0.9986,  0.0486,  0.9999, -0.0130,  1.0000,  0.9984,  0.9997,\n",
      "          0.0088,  1.0000, -0.0228],\n",
      "        [ 1.0000,  0.9968,  0.1560,  1.0000, -0.0146,  1.0000,  0.9982,  0.9997,\n",
      "          0.9998,  1.0000, -0.0259],\n",
      "        [ 1.0000,  0.9986,  0.9994,  0.9996, -0.0155,  1.0000,  0.9984,  0.9998,\n",
      "          0.0088,  1.0000,  0.9994],\n",
      "        [ 1.0000,  0.9986, -0.0107,  1.0000, -0.0131,  1.0000,  0.9981,  0.9997,\n",
      "          0.9998,  1.0000, -0.0246],\n",
      "        [ 1.0000, -0.0439, -0.0192,  1.0000, -0.0146,  1.0000,  0.9984,  0.5517,\n",
      "          0.0088,  1.0000, -0.0254],\n",
      "        [ 1.0000, -0.0452,  0.9994,  0.9999, -0.0151,  1.0000,  0.9979,  0.9995,\n",
      "          0.9998,  1.0000,  0.9994],\n",
      "        [ 1.0000, -0.0315,  0.9994, -0.1487, -0.0339,  0.3361,  0.9984, -0.3843,\n",
      "          0.9998,  1.0000,  0.9992],\n",
      "        [ 1.0000, -0.0424, -0.0086,  1.0000, -0.0127,  1.0000,  0.9983,  0.9997,\n",
      "          0.9998,  1.0000, -0.0283],\n",
      "        [ 1.0000,  0.9986,  0.9994,  0.9997, -0.0165,  1.0000,  0.9984,  0.9991,\n",
      "          0.0084,  1.0000, -0.0222],\n",
      "        [ 1.0000, -0.0423,  0.9994,  1.0000, -0.0165,  1.0000,  0.9647,  0.9996,\n",
      "          0.0087,  1.0000,  0.9994],\n",
      "        [ 1.0000,  0.9986, -0.0037,  0.9997, -0.0143,  1.0000,  0.9984,  0.9997,\n",
      "          0.0088,  1.0000, -0.0262],\n",
      "        [ 1.0000, -0.0445, -0.0034,  1.0000, -0.0125,  1.0000,  0.9983,  0.9997,\n",
      "          0.0088,  1.0000, -0.0263],\n",
      "        [ 1.0000, -0.0419, -0.0049,  0.9997, -0.0153,  1.0000,  0.9893,  0.9997,\n",
      "          0.0088,  1.0000, -0.0218],\n",
      "        [ 1.0000,  0.9986,  0.9994,  1.0000, -0.0158,  1.0000,  0.9981,  0.9997,\n",
      "          0.9998,  1.0000, -0.0240],\n",
      "        [ 1.0000,  0.9986, -0.0034,  1.0000, -0.0125,  1.0000,  0.9979,  0.9997,\n",
      "          0.9998,  1.0000, -0.1248],\n",
      "        [ 1.0000,  0.9986, -0.0255,  0.9998, -0.0127,  1.0000,  0.9981,  0.9165,\n",
      "          0.9998,  1.0000, -0.0237],\n",
      "        [ 1.0000, -0.0453, -0.0045,  0.9999, -0.0128,  1.0000,  0.9844,  0.9996,\n",
      "          0.0088,  1.0000, -0.0220],\n",
      "        [ 1.0000,  0.2002,  0.9994, -0.1574, -0.0186,  0.1971,  0.9982,  0.9997,\n",
      "          0.0087,  1.0000, -0.0249],\n",
      "        [ 1.0000, -0.0409, -0.0032,  1.0000, -0.0142,  1.0000,  0.9983,  0.9997,\n",
      "          0.0088,  1.0000, -0.0303],\n",
      "        [ 1.0000,  0.9721,  0.9994,  0.1874, -0.0172,  0.9945,  0.9984,  0.9997,\n",
      "          0.0088,  1.0000,  0.9863],\n",
      "        [ 1.0000,  0.9981, -0.0438,  1.0000, -0.0142,  1.0000,  0.9980,  0.9997,\n",
      "          0.0085,  1.0000, -0.0725],\n",
      "        [ 1.0000,  0.9986,  0.0012,  1.0000, -0.0136,  1.0000,  0.9983,  0.9997,\n",
      "          0.9998,  1.0000, -0.0247],\n",
      "        [ 1.0000,  0.9986,  0.9994,  0.9999, -0.0153,  1.0000,  0.9984,  0.9997,\n",
      "          0.0087,  1.0000, -0.0209]], grad_fn=<CatBackward>)\n",
      "tensor([[1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0],\n",
      "        [1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0],\n",
      "        [1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.9873],\n",
       "        [0.9902],\n",
       "        [0.9868],\n",
       "        [0.9911],\n",
       "        [0.9828],\n",
       "        [0.9841],\n",
       "        [0.9899],\n",
       "        [0.9935],\n",
       "        [0.9879],\n",
       "        [0.9876],\n",
       "        [0.9885],\n",
       "        [0.9862],\n",
       "        [0.9885],\n",
       "        [0.9669],\n",
       "        [0.9879],\n",
       "        [0.9904],\n",
       "        [0.9818],\n",
       "        [0.9885],\n",
       "        [0.9900],\n",
       "        [0.9911],\n",
       "        [0.9716],\n",
       "        [0.9901],\n",
       "        [0.9890],\n",
       "        [0.9899],\n",
       "        [0.9854],\n",
       "        [0.9891],\n",
       "        [0.9903],\n",
       "        [0.9823],\n",
       "        [0.9891],\n",
       "        [0.9884],\n",
       "        [0.9718],\n",
       "        [0.9874],\n",
       "        [0.9873],\n",
       "        [0.9871],\n",
       "        [0.9886],\n",
       "        [0.9861],\n",
       "        [0.9883],\n",
       "        [0.9903],\n",
       "        [0.9905],\n",
       "        [0.9902],\n",
       "        [0.9904],\n",
       "        [0.9789],\n",
       "        [0.9875],\n",
       "        [0.9930],\n",
       "        [0.9880],\n",
       "        [0.9905],\n",
       "        [0.9879],\n",
       "        [0.9881],\n",
       "        [0.9908],\n",
       "        [0.9846],\n",
       "        [0.9817],\n",
       "        [0.9875],\n",
       "        [0.9901],\n",
       "        [0.9883],\n",
       "        [0.9904],\n",
       "        [0.9824],\n",
       "        [0.9869],\n",
       "        [0.9903],\n",
       "        [0.9856],\n",
       "        [0.9866],\n",
       "        [0.9905],\n",
       "        [0.9901],\n",
       "        [0.9903],\n",
       "        [0.9875],\n",
       "        [0.9886],\n",
       "        [0.9906],\n",
       "        [0.9889],\n",
       "        [0.9808],\n",
       "        [0.9883],\n",
       "        [0.9889],\n",
       "        [0.9884],\n",
       "        [0.9885],\n",
       "        [0.9889],\n",
       "        [0.9863],\n",
       "        [0.9890],\n",
       "        [0.9865],\n",
       "        [0.9827],\n",
       "        [0.9848],\n",
       "        [0.9894],\n",
       "        [0.9822],\n",
       "        [0.9819],\n",
       "        [0.9845],\n",
       "        [0.9902],\n",
       "        [0.9858],\n",
       "        [0.9906],\n",
       "        [0.9902],\n",
       "        [0.9876],\n",
       "        [0.9892],\n",
       "        [0.9885],\n",
       "        [0.9876],\n",
       "        [0.9892],\n",
       "        [0.9908],\n",
       "        [0.9696],\n",
       "        [0.9881],\n",
       "        [0.9898],\n",
       "        [0.9900],\n",
       "        [0.9901],\n",
       "        [0.9889],\n",
       "        [0.9902],\n",
       "        [0.9904],\n",
       "        [0.9896],\n",
       "        [0.9796],\n",
       "        [0.9901],\n",
       "        [0.9674],\n",
       "        [0.9894],\n",
       "        [0.9890],\n",
       "        [0.9812],\n",
       "        [0.9826],\n",
       "        [0.9874],\n",
       "        [0.9886],\n",
       "        [0.9825],\n",
       "        [0.9849],\n",
       "        [0.9899],\n",
       "        [0.9871],\n",
       "        [0.9911],\n",
       "        [0.9873],\n",
       "        [0.9912],\n",
       "        [0.9904],\n",
       "        [0.9903],\n",
       "        [0.9885],\n",
       "        [0.9883],\n",
       "        [0.9864],\n",
       "        [0.9763],\n",
       "        [0.9872],\n",
       "        [0.9885],\n",
       "        [0.9878],\n",
       "        [0.9900],\n",
       "        [0.9901],\n",
       "        [0.9903],\n",
       "        [0.9852],\n",
       "        [0.9902],\n",
       "        [0.9895],\n",
       "        [0.9951],\n",
       "        [0.9904],\n",
       "        [0.9899],\n",
       "        [0.9655],\n",
       "        [0.9894],\n",
       "        [0.9844],\n",
       "        [0.9874],\n",
       "        [0.9860],\n",
       "        [0.9853],\n",
       "        [0.9895],\n",
       "        [0.9850],\n",
       "        [0.9906],\n",
       "        [0.9852],\n",
       "        [0.9779],\n",
       "        [0.9866],\n",
       "        [0.9903],\n",
       "        [0.9866],\n",
       "        [0.9826],\n",
       "        [0.9826],\n",
       "        [0.9816],\n",
       "        [0.9882],\n",
       "        [0.9930],\n",
       "        [0.9827],\n",
       "        [0.9742],\n",
       "        [0.9825],\n",
       "        [0.9912],\n",
       "        [0.9904],\n",
       "        [0.9847],\n",
       "        [0.9887],\n",
       "        [0.9903],\n",
       "        [0.9873],\n",
       "        [0.9769],\n",
       "        [0.9902],\n",
       "        [0.9857],\n",
       "        [0.9902],\n",
       "        [0.9875],\n",
       "        [0.9901],\n",
       "        [0.9876],\n",
       "        [0.9943],\n",
       "        [0.9903],\n",
       "        [0.9886],\n",
       "        [0.9904],\n",
       "        [0.9885],\n",
       "        [0.9895],\n",
       "        [0.9901],\n",
       "        [0.9808],\n",
       "        [0.9832],\n",
       "        [0.9833],\n",
       "        [0.9910],\n",
       "        [0.9809],\n",
       "        [0.9878],\n",
       "        [0.9822],\n",
       "        [0.9886],\n",
       "        [0.9778],\n",
       "        [0.9886],\n",
       "        [0.9888],\n",
       "        [0.9790],\n",
       "        [0.9903],\n",
       "        [0.9832],\n",
       "        [0.9894],\n",
       "        [0.9903],\n",
       "        [0.9876],\n",
       "        [0.9862],\n",
       "        [0.9866],\n",
       "        [0.9874],\n",
       "        [0.9878],\n",
       "        [0.9894],\n",
       "        [0.9860],\n",
       "        [0.9834],\n",
       "        [0.9904],\n",
       "        [0.9894],\n",
       "        [0.9712],\n",
       "        [0.9874],\n",
       "        [0.9868],\n",
       "        [0.9894],\n",
       "        [0.9902],\n",
       "        [0.9820],\n",
       "        [0.9539],\n",
       "        [0.9889],\n",
       "        [0.9905],\n",
       "        [0.9902],\n",
       "        [0.9893],\n",
       "        [0.9901],\n",
       "        [0.9873],\n",
       "        [0.9792],\n",
       "        [0.9905],\n",
       "        [0.9877],\n",
       "        [0.9898],\n",
       "        [0.9911],\n",
       "        [0.9865],\n",
       "        [0.9860],\n",
       "        [0.9833],\n",
       "        [0.9776],\n",
       "        [0.9894],\n",
       "        [0.9905],\n",
       "        [0.9899],\n",
       "        [0.9923],\n",
       "        [0.9880],\n",
       "        [0.9822],\n",
       "        [0.9649],\n",
       "        [0.9799],\n",
       "        [0.9789],\n",
       "        [0.9882],\n",
       "        [0.9902],\n",
       "        [0.9188],\n",
       "        [0.9901],\n",
       "        [0.9711],\n",
       "        [0.9848],\n",
       "        [0.9843],\n",
       "        [0.9878],\n",
       "        [0.9893],\n",
       "        [0.9854],\n",
       "        [0.9900],\n",
       "        [0.9906],\n",
       "        [0.9903],\n",
       "        [0.9894],\n",
       "        [0.9879],\n",
       "        [0.9873],\n",
       "        [0.9905],\n",
       "        [0.9868],\n",
       "        [0.9861],\n",
       "        [0.9878],\n",
       "        [0.9864],\n",
       "        [0.9902]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = generator(torch.randn(30, 11))\n",
    "print(f)\n",
    "print(f.ge(0.5).long())\n",
    "dis = discriminator_1(generator(torch.randn(batch, 11)))\n",
    "dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    " for g_index in range(32):\n",
    "    # Train G on D's response\n",
    "    generator.zero_grad()\n",
    "    g_fake_data = generator(torch.randn(batch, 11))\n",
    "    d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "    g_fake_lable = torch.ones(batch)\n",
    "    g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "    g_loss.backward()\n",
    "    generator_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: D (0.22125402092933655 real_err, 0.059217192232608795 fake_err) G_l (0.015695534646511078 err) G_0l (None) G_1l (None) G_2l (0.07190127670764923) G_3l (0.07072243094444275);\n",
      "Epoch 1: D (0.8784480094909668 real_err, 0.14986123144626617 fake_err) G_l (0.05311468243598938 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.04634556174278259);\n",
      "Epoch 2: D (0.32248008251190186 real_err, 0.18921847641468048 fake_err) G_l (0.010749738663434982 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.06491663306951523);\n",
      "Epoch 3: D (0.2830143868923187 real_err, 0.25805848836898804 fake_err) G_l (0.02052653394639492 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.029447432607412338);\n",
      "Epoch 4: D (0.08148318529129028 real_err, 0.15486261248588562 fake_err) G_l (0.0037686177529394627 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.09675496816635132);\n",
      "Epoch 5: D (0.6273917555809021 real_err, 0.5055474638938904 fake_err) G_l (0.02917487919330597 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.030271584168076515);\n",
      "Epoch 6: D (0.49867790937423706 real_err, 0.5175061225891113 fake_err) G_l (0.3499322235584259 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.0468820184469223);\n",
      "Epoch 7: D (0.4130099415779114 real_err, 0.2603578567504883 fake_err) G_l (0.0016336796106770635 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.08965965360403061);\n",
      "Epoch 8: D (0.5100611448287964 real_err, 0.34230339527130127 fake_err) G_l (0.10457960516214371 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.01652887836098671);\n",
      "Epoch 9: D (0.5397396683692932 real_err, 0.16634927690029144 fake_err) G_l (0.0007370166131295264 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.028134718537330627);\n",
      "Epoch 10: D (0.4859308898448944 real_err, 0.34623533487319946 fake_err) G_l (0.00014762203500140458 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.32949814200401306);\n",
      "Epoch 11: D (0.2175847291946411 real_err, 0.17840856313705444 fake_err) G_l (0.06142515689134598 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.25313279032707214);\n",
      "Epoch 12: D (0.5938618183135986 real_err, 0.30312445759773254 fake_err) G_l (0.0015326347202062607 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.5732567310333252);\n",
      "Epoch 13: D (0.3207414150238037 real_err, 0.13182073831558228 fake_err) G_l (0.05894848331809044 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.5374412536621094);\n",
      "Epoch 14: D (0.16929370164871216 real_err, 0.1976919323205948 fake_err) G_l (0.47929638624191284 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.3470424711704254);\n",
      "Epoch 15: D (0.11231118440628052 real_err, 0.048068709671497345 fake_err) G_l (4.9841088184621185e-05 err) G_0l (None) G_1l (None) G_2l (None) G_3l (0.6881944537162781);\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-3e16d4c23e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mg_fake_lable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_g_fake_decision\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_fake_lable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mgenerator_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import copy\n",
    "for epoch in range(num_epochs):\n",
    "    # GAN1\n",
    "    # 1. Train D on real+fake\n",
    "    # D.zero_grad()\n",
    "    data = copy.copy(dataLoader)\n",
    "\n",
    "    for real_data in data:\n",
    "\n",
    "        # 1A: Train D1 on real\n",
    "        discriminator_1.zero_grad()\n",
    "        d_real_data = real_data\n",
    "        # real data's lable should be true\n",
    "        d_real_labe = torch.ones(d_real_data.size()[0])\n",
    "        d_real_decision = discriminator_1(d_real_data.float())\n",
    "        d_real_loss = criterion(\n",
    "            torch.squeeze(d_real_decision), d_real_labe)\n",
    "        d_real_loss.backward()\n",
    "\n",
    "        # 1B: Train D1 on fake data\n",
    "        d_fake_data = generator(torch.randn(batch, 11))\n",
    "        # print(d_fake_data.size())\n",
    "        d_fake_lable = torch.zeros(batch)\n",
    "        d_fake_decision = discriminator_1(d_fake_data)\n",
    "        d_fake_loss = criterion(torch.squeeze(\n",
    "            d_fake_decision), d_fake_lable)\n",
    "        d_fake_loss.backward()\n",
    "        # Only optimizes D1's parameters\n",
    "        discriminator_1_optim.step()\n",
    "\n",
    "        drl, dfl = d_real_loss.tolist(), d_fake_loss.tolist()\n",
    "    for g_index in range(g_steps):\n",
    "        # Train G on D's response\n",
    "        generator.zero_grad()\n",
    "\n",
    "        g_fake_data = generator(torch.randn(batch, 11))\n",
    "        d_g_fake_decision = discriminator_1(g_fake_data)\n",
    "        g_fake_lable = torch.ones(batch)\n",
    "        g_loss = criterion(torch.squeeze(d_g_fake_decision), g_fake_lable)\n",
    "        g_loss.backward()\n",
    "        generator_optim.step()\n",
    "        gl = g_loss.tolist()\n",
    "\n",
    "    # GAN2\n",
    "    for g_index in range(g2_steps):\n",
    "        generator.zero_grad()\n",
    "        noise_z = torch.randn(batch, 11)\n",
    "        fake_data = generator(noise_z)\n",
    "        # O = {race; native country}:(0,0) (0,1) (1,0) (1,1)\n",
    "        noise_o0 = []\n",
    "        noise_o1 = []\n",
    "        noise_o2 = []\n",
    "        noise_o3 = []\n",
    "\n",
    "        for index, single_data in enumerate(fake_data):\n",
    "            if(single_data[7] < 0.5 and single_data[9] < 0.5):\n",
    "                noise_o0.append(noise_z[index].view(1, -1))\n",
    "            elif(single_data[7] < 0.5 and single_data[9] >= 0.5):\n",
    "                noise_o1.append(noise_z[index].view(1, -1))\n",
    "            elif(single_data[7] >= 0.5 and single_data[9] < 0.5):\n",
    "                noise_o2.append(noise_z[index].view(1, -1))\n",
    "            else:\n",
    "                noise_o3.append(noise_z[index].view(1, -1))\n",
    "        ge0,ge1,ge2,ge3 = None,None,None,None\n",
    "        if(len(noise_o0) != 0):\n",
    "            noise_o0 = torch.cat(noise_o0)\n",
    "            o0_0_lable = generator(noise_o0, 0)[:, -1]\n",
    "            o0_1_lable = generator(noise_o0, 1)[:, -1].detach()\n",
    "            g_error0 = criterion(o0_0_lable, o0_1_lable)\n",
    "            g_error0.backward()\n",
    "            ge0 = g_error0.tolist()\n",
    "        if(len(noise_o1) != 0):\n",
    "            noise_o1 = torch.cat(noise_o1)\n",
    "            o1_0_lable = generator(noise_o1, 0)[:, -1]\n",
    "            o1_1_lable = generator(noise_o1, 1)[:, -1].detach()\n",
    "            g_error1 = criterion(o1_0_lable, o1_1_lable)\n",
    "            g_error1.backward()\n",
    "            ge1 = g_error1.tolist()\n",
    "        if(len(noise_o2) != 0):\n",
    "            noise_o2 = torch.cat(noise_o2)\n",
    "            o2_0_lable = generator(noise_o2, 0)[:, -1]\n",
    "            o2_1_lable = generator(noise_o2, 1)[:, -1].detach()\n",
    "            g_error2 = criterion(o2_0_lable, o2_1_lable)\n",
    "            g_error2.backward()\n",
    "            ge2 = g_error2.tolist()\n",
    "        if(len(noise_o3) != 0):\n",
    "            noise_o3 = torch.cat(noise_o3)\n",
    "            o3_0_lable = generator(noise_o3, 0)[:, -1]\n",
    "            o3_1_lable = generator(noise_o3, 1)[:, -1].detach()\n",
    "            g_error3 = criterion(o3_0_lable, o3_1_lable)\n",
    "            g_error3.backward()\n",
    "            ge3 = g_error3.tolist()\n",
    "\n",
    "        generator_optim.step()\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"Epoch %s: D (%s real_err, %s fake_err) G_l (%s err) G_0l (%s) G_1l (%s) G_2l (%s) G_3l (%s);\" % (\n",
    "            epoch, drl, dfl, gl, ge0, ge1, ge2, ge3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': conda)",
   "language": "python",
   "name": "python37464bitanaconda3conda5df9da8887984ee4b635526a125f85b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
